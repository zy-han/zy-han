{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter Grader\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![data-x](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### NAME: Zhiyang Han\n",
    "\n",
    "#### STUDENT ID: 3036337667\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  HW3-4: NLP (Text Processing and  Feature Engineering & Text Representation)\n",
    "**(Total 120 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for Sentiment Analysis on IMDB Movie Reviews\n",
    "\n",
    "In this assignment we will be exploring tools for Natural Language Processing (NLP). Our task is sentiment analysis for movie reviews and in that context we will touch upon multiple areas:\n",
    "\n",
    "- Feature engineering\n",
    "- Bag of words modeling\n",
    "- Word2Vec modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to install the packages you need for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.8/site-packages (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.6.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.8/site-packages (from gensim) (4.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to load the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /opt/conda/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /opt/conda/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /opt/conda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /opt/conda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/opt/conda/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# import Beautiful Soup, NumPy and Pandas, etc\n",
    "import bs4 as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    " \n",
    "# download NLTK classifiers - these are cached locally on your machine\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# import ml classifiers\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "from nltk.stem import PorterStemmer     # parsing/stemmer\n",
    "from nltk.tag import pos_tag            # parts-of-speech tagging\n",
    "from nltk.corpus import wordnet         # sentiment scores\n",
    "from nltk.stem import WordNetLemmatizer # stem and context\n",
    "from nltk.corpus import stopwords       # stopwords\n",
    "from nltk.util import ngrams            # ngram iterator\n",
    "\n",
    "# import word2vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "**(Total 70 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "___\n",
    "\n",
    "### Data Description\n",
    ">Data source: https://www.kaggle.com/c/word2vec-nlp-tutorial/data (originally from [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/))<br>\n",
    ">\n",
    ">Data Description:<br><br>\n",
    ">We will be using Kaggle's **Bag of Words Meets Bags of Popcorn** dataset to explore [IMBD](https://www.imdb.com/) movie review data.  Labeled training dataset consists of 25,000 IMDB movie reviews. The sentiment of the reviews are binary, meaning an IMDB rating < 5 results in a sentiment score of 0, and a rating >=7 have a sentiment score of 1 (no reviews with score 5 or 6 are included in the analysis). No individual movie has more than 30 reviews. The training data set is constructed in a balanced way so that there are an equal number of positive and negative reviews for each movie.\n",
    ">\n",
    ">Data Set:<br>\n",
    ">* ```labeledTrainData.tsv``` --> The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id (numerical), sentiment (categorical), and text for each review (textual).<br>\n",
    ">\n",
    ">\n",
    "> Further Reading:<br>\n",
    "> \n",
    "> * [Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 5 rows\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.a Clean function\n",
    "**(Total 40 points)**\n",
    "\n",
    "Finish the function `review_cleaner` to preprocess reviews. Here is an overview of what it does:\n",
    "\n",
    "> - Removes HTML tags (using beautifulsoup)\n",
    "> - Extract emoticons (emotion symbols, aka smileys :D )\n",
    "> - Removes non-letters (using regular expression)\n",
    "> - Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n",
    "> - Removes all the English stopwords from the list of movie review words\n",
    "> - Applies either stemming or lemmatization, as indicated by the arguments\n",
    "> - Join the words back into one string seperated by space, append the emoticons to the end\n",
    "\n",
    "More details can be found in the introduction [slides](https://datax.berkeley.edu/wp-content/uploads/2020/06/Module-1_-Preprocessing-Slides.pdf).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_a_0\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def review_cleaner(review, lemmatize=True, stem=False):\n",
    "    '''\n",
    "        Clean and preprocess a review.\n",
    "            1. Remove HTML tags\n",
    "            2. Extract emoticons\n",
    "            3. Use regex to remove all special characters (only keep letters)\n",
    "            4. Make strings to lower case and tokenize / word split reviews\n",
    "            5. Remove English stopwords\n",
    "            6. Lemmatize\n",
    "            7. Rejoin to one string\n",
    "        \n",
    "        @review (type:str) is an unprocessed review string\n",
    "        @return (type:str) is a 6-step preprocessed review string\n",
    "    '''\n",
    "\n",
    "    \n",
    "\n",
    "    if lemmatize == True and stem == True:\n",
    "        raise RuntimeError(\"May not pass both lemmatize and stem flags\")\n",
    "\n",
    "    #1. Remove HTML tags\n",
    "    \n",
    "    text = bs.BeautifulSoup(review, \"html.parser\").get_text()\n",
    "\n",
    "    #2. Use regex to find emoticons\n",
    "    \n",
    "    #noemoticon = re.sub('>?[\\:\\;X][\\-=]*[3\\)D\\(>sp}]', '', text)\n",
    "    regex = '>?[\\:\\;X][\\-=]*[3\\)D\\(>sp}]'\n",
    "    match = re.findall(regex, text)\n",
    "    match = ''.join([str(m) for m in match])\n",
    "    #regex = re.compile(r'>?[\\:\\;X][\\-=]*[3\\)D\\(>sp}]')\n",
    "    #match = regex.match(review)\n",
    "\n",
    "    #3. Remove punctuation\n",
    "    \n",
    "    #nopuncreview = re.sub('[^\\w\\s'+match+']', '', text)\n",
    "    nopuncreview = re.sub('[^\\w\\s]', '', text)\n",
    "\n",
    "    #4. Tokenize into words (all lower case)\n",
    "    \n",
    "    token = sent_tokenize(nopuncreview)\n",
    "    \n",
    "    review = [x.lower() for x in token]\n",
    "\n",
    "    #5. Remove stopwords, Lemmatize, Stem\n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    words = [word_tokenize(s) for s in review][0]\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_sentence = []\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    \n",
    "    if lemmatize == True:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stemmer = PorterStemmer()\n",
    "        lemma = []\n",
    "        for i in range(len(filtered_sentence)):\n",
    "            lem = [lemmatizer.lemmatize(f) for f in filtered_sentence[i]]\n",
    "            lem = ''.join([str(i) for i in lem])\n",
    "            lemma.append(lem)\n",
    "        lemma = ' '.join([str(j) for j in lemma])\n",
    "        lemmastr = ' '.join([lemmatizer.lemmatize(words) for words in word_tokenize(lemma)])\n",
    "    else:\n",
    "        lemmastr = ' '.join(filtered_sentence)\n",
    "    \n",
    "    if stem == True:\n",
    "        ps = PorterStemmer()\n",
    "        stem = []\n",
    "        for word in word_tokenize(lemma):\n",
    "            stem.append(ps.stem(word))\n",
    "        stemstr = \" \".join(stem)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #6. Join the review to one sentence\n",
    "    \n",
    "    if stem == True:\n",
    "        if match is None:\n",
    "            review_processed = stemstr.rstrip()\n",
    "        else:\n",
    "            seq = [stemstr, match]\n",
    "            review_processed = \" \".join(seq).rstrip()\n",
    "    else:\n",
    "        if match is None:\n",
    "            review_processed = lemmastr.rstrip()\n",
    "        else:\n",
    "            seq = [lemmastr, match]\n",
    "            review_processed = \" \".join(seq).rstrip()\n",
    "    \n",
    "    return review_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q1_a_0</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q1_a_0 passed!"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1_a_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_a_1\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for HTML Tags (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q1_a_1</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q1_a_1 passed!"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1_a_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_a_2\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for emoticons (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q1_a_2</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q1_a_2 passed!"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1_a_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_a_3\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for stopwords, Lemmatize, Stem (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q1_a_3</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q1_a_3 passed!"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1_a_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b Set up your review\n",
    "**(Total 0 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To make things interesting, everyone gets to analyze a different review. Set `seed_value` to your favorite number, your name, or whatever else you'd like.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_b\n",
    "manual: false\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 'han'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q1_b</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q1_b passed!"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I can't believe that they took this off the air. Especially, when they only had a few more episodes left. My daughter, sister and a few of my friends loved watching this show. We were so upset when they stopped showing this because of so called ratings. It is not fair to the people who were watching this show since the beginning. We had a right to see the end. I wish they would take an overall vote from all people with a 3 times a year voting system. They could send out papers in the mail and we as viewers could give an overall vote on all programs that we watch or have heard about. This could also help promote a new show. People would see it and wonder what it is. Not only could you see what the viewers are watching, you could also use this as a tool for free advertisement for TV and cable channels. We want to see the other episodes. Bring it back!!\"\n",
      "cant believe took air especially episode left daughter sister friend loved watching show upset stopped showing called rating fair people watching show since beginning right see end wish would take overall vote people 3 time year voting system could send paper mail viewer could give overall vote program watch heard could also help promote new show people would see wonder could see viewer watching could also use tool free advertisement tv cable channel want see episode bring back\n"
     ]
    }
   ],
   "source": [
    "# Print out a cleaned version of the randomly selected review\n",
    "my_review_id = int(hashlib.md5(str(seed_value).encode(\"utf-8\")).hexdigest()[:8], 16) % len(train.index)\n",
    "my_review = train.iloc[my_review_id][\"review\"]\n",
    "print(my_review)\n",
    "print(review_cleaner(my_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c Find the stopwords\n",
    "\n",
    "**(Total 10 points)**\n",
    "\n",
    "Find the first 5 stopwords in your chosen review. \n",
    "\n",
    "First review the list of stopwords below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i me my myself we our ours ourselves you you're you've you'll you'd your yours yourself yourselves he him his himself she she's her hers herself it it's its itself they them their theirs themselves what which who whom this that that'll these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don don't should should've now d ll m o re ve y ain aren aren't couldn couldn't didn didn't doesn doesn't hadn hadn't hasn hasn't haven haven't isn isn't ma mightn mightn't mustn mustn't needn needn't shan shan't shouldn shouldn't wasn wasn't weren weren't won won't wouldn wouldn't\n",
      "['that', 'they', 'this', 'off', 'the', 'when', 'they', 'only', 'had', 'a', 'few', 'more', 'and', 'a', 'few', 'of', 'my', 'this', 'were', 'so', 'when', 'they', 'this', 'because', 'of', 'so', 'is', 'not', 'to', 'the', 'who', 'were', 'this', 'the', 'had', 'a', 'to', 'the', 'they', 'an', 'from', 'all', 'with', 'a', 'a', 'out', 'in', 'the', 'and', 'we', 'as', 'an', 'on', 'all', 'that', 'we', 'or', 'have', 'about', 'a', 'it', 'and', 'what', 'it', 'is', 'only', 'you', 'what', 'the', 'are', 'you', 'this', 'as', 'a', 'for', 'for', 'and', 'to', 'the', 'other', 'it']\n"
     ]
    }
   ],
   "source": [
    "# See what the stopwords are\n",
    "print(\" \".join(stopwords.words(\"english\")))\n",
    "from nltk.tokenize import word_tokenize\n",
    "mystopwords = [word for word in word_tokenize(my_review) if word in stopwords.words(\"english\")]\n",
    "print(mystopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "For your selected review, find the 5 first stopwords. Store them in the list named `first_5_stopwords` in the order in which they appear in the review.\n",
    "\n",
    "e.g., \n",
    "```\n",
    "first_5_stopwords = ['having', 'the', 'to', 'some', 'of']\n",
    "```\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_c\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that', 'they', 'this', 'off', 'the']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_5_stopwords = ['that', 'they', 'this', 'off', 'the']\n",
    "first_5_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q1_c</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q1_c passed!"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.d Lemmatization\n",
    "\n",
    "**(Total 10 points)**\n",
    "\n",
    "Lemmatization allows grouping of common forms of a word.\n",
    "\n",
    "Here are some examples of lemmatization:\n",
    "* images -> image\n",
    "* waxworks -> waxwork\n",
    "* sweets -> sweet\n",
    "\n",
    "Find the first 3 words in `my_review` that are lemmatized. Store them in the list named `first_3_lemmatized` in the order in which they appear in the review.\n",
    "\n",
    "E.g.:\n",
    "```\n",
    "first_3_lemmatized = ['images', 'waxworks', 'sweets']\n",
    "```\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_d\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization examples:\n",
      "episodes -> episode\n",
      "friends -> friend\n",
      "ratings -> rating\n"
     ]
    }
   ],
   "source": [
    "first_3_lemmatized = ['episodes', 'friends', 'ratings']\n",
    "\n",
    "print(\"Lemmatization examples:\")\n",
    "for w in first_3_lemmatized:\n",
    "    print(\"{} -> {}\".format(w, wnl.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q1_d</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q1_d passed!"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1_d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.e Stemming\n",
    "\n",
    "**(Total 10 points)**\n",
    "\n",
    "Stemming allows grouping of common forms of a word.\n",
    "\n",
    "Here are some examples of stemming:\n",
    "* nonsense -> nonsens\n",
    "* investigates -> investig\n",
    "* disappearance -> disappear\n",
    "\n",
    "Find the first 3 words in `my_review` that are modified by stemming. Store them in the list named `first_3_stemmed` in the order in which they appear in the review.\n",
    "\n",
    "E.g.:\n",
    "```\n",
    "first_3_stemmed = ['nonsense', 'investigates', 'disappearance']\n",
    "```\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_e\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cant believ took air especi episod left daughter sister friend love watch show upset stop show call rate fair peopl watch show sinc begin right see end wish would take overal vote peopl 3 time year vote system could send paper mail viewer could give overal vote program watch heard could also help promot new show peopl would see wonder could see viewer watch could also use tool free advertis tv cabl channel want see episod bring back\n",
      "Stemming examples:\n",
      "believe -> believ\n",
      "especially -> especi\n",
      "episodes -> episod\n"
     ]
    }
   ],
   "source": [
    "cleaned = review_cleaner(my_review)\n",
    "ps = PorterStemmer()\n",
    "stem = []\n",
    "from nltk.tokenize import word_tokenize\n",
    "for word in word_tokenize(cleaned):\n",
    "    stem.append(ps.stem(word))\n",
    "    stemstr = \" \".join(stem)\n",
    "print(stemstr)\n",
    "\n",
    "first_3_stemmed = ['believe', 'especially', 'episodes']\n",
    "\n",
    "print(\"Stemming examples:\")\n",
    "for w in first_3_stemmed:\n",
    "    print(\"{} -> {}\".format(w, ps.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q1_e</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q1_e passed!"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1_e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "___\n",
    "\n",
    "## 2. Train and Validate a Sentiment Analysis Model using a Random Forest Classifier\n",
    "**(Total 30 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we have written the code to train the classifier for you. Your task will be to explore its performance characteristics with your own movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We vectorize the text using a bag of words model\n",
    "def get_vectorizer(ngram, max_features):\n",
    "    return CountVectorizer(ngram_range=(1, ngram),\n",
    "                             analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = review_cleaner,\n",
    "                             stop_words = None, \n",
    "                             max_features = max_features)\n",
    "\n",
    "# Model training\n",
    "def train_predict_sentiment(reviews, vectorizer, y=train[\"sentiment\"], ngram=1, max_features=1000, model_random_state=0):\n",
    "    '''\n",
    "        This function will:\n",
    "            1. split data into train and test set.\n",
    "            2. get n-gram counts from cleaned reviews \n",
    "            3. train a random forest model using train n-gram counts and y (labels)\n",
    "            4. test the model on your test split\n",
    "            5. print accuracy of sentiment prediction on test and training data\n",
    "            6. print confusion matrix on test data results\n",
    "\n",
    "            To change n-gram type, set value of ngram argument\n",
    "            To change the number of features you want the countvectorizer to generate, set the value of max_features argument\n",
    "            \n",
    "            @cleaned_review (type:str) is preprocessed string from review_cleaner()\n",
    "            @return none\n",
    "    '''\n",
    "\n",
    "    print(\"Creating the model!\\n\")\n",
    "    \n",
    "    # train / test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reviews, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarray() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train)\n",
    "    if not isinstance(train_bag, np.ndarray):\n",
    "        train_bag = train_bag.toarray()\n",
    "    test_bag = vectorizer.transform(X_test)\n",
    "    if not isinstance(test_bag, np.ndarray):\n",
    "        test_bag = test_bag.toarray()\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 50 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50, random_state = model_random_state) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "    # predict\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    # validation\n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    print(\" The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    print()\n",
    "    print('CONFUSION MATRIX:')\n",
    "    print('         Predicted')\n",
    "    print('          neg pos')\n",
    "    print(' Actual')\n",
    "    c=confusion_matrix(y_test, test_predictions)\n",
    "    print('     neg  ',c[0])\n",
    "    print('     pos  ',c[1])\n",
    "\n",
    "    return forest\n",
    "\n",
    "# Print out the top features\n",
    "def top_features(forest, vectorizer, n):\n",
    "    #Extract feature importance\n",
    "    print('\\nTOP TEN IMPORTANT FEATURES:')\n",
    "    feature_text = vectorizer.get_feature_names().copy()\n",
    "    feature_importance = forest.feature_importances_.copy()\n",
    "    \n",
    "    indices = np.argsort(feature_importance)[::-1]\n",
    "    \n",
    "    top_n_ind = indices[:n]\n",
    "    top_n = list([vectorizer.get_feature_names()[ind] for ind in top_n_ind])\n",
    "    \n",
    "    return top_n\n",
    "\n",
    "# Print out whether the prediction is accurate\n",
    "def check_prediction(model, vectorizer, review, expected):\n",
    "    prediction = model.predict(vectorizer.transform([review]))[0]\n",
    "    sentiment = \"👍\" if prediction else \"👎\"\n",
    "    correct = \"\\x1b[92mcorrect\\x1b[0m\" if prediction == expected else \"\\x1b[31mincorrect\\x1b[0m\"\n",
    "    print(\"{} ⟶ {} {}\".format(review, sentiment, correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "### 2.a Train Random Forest Classifier Model\n",
    "\n",
    "**(Total 15 points)**\n",
    "\n",
    "Use the above functions to train your random forest model. Set `ngram=1`, `max_features=100` for the `get_vectorizer` function, then use  `train_predict_sentiment` function to train your model using the train dataset. Finally, use `top_features` function to print the top 10 features. This cell may take a few minutes to run.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "manual: false\n",
    "points: 15\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.99995 \n",
      " The validation accuracy is:  0.723\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [1856  692]\n",
      "     pos   [ 693 1759]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'great', 'movie', 'film', 'one', 'best', 'even', 'love', 'like', 'good']\n"
     ]
    }
   ],
   "source": [
    "# Train RFC model\n",
    "reviews = []\n",
    "for i in range(len(train)):\n",
    "    review = review_cleaner(train['review'][i])\n",
    "    reviews.append(review)\n",
    "training = pd.DataFrame(reviews, columns = ['reviews'])\n",
    "\n",
    "vectorizer = get_vectorizer(1, 100)\n",
    "forest_model = train_predict_sentiment(training['reviews'], vectorizer, y=train[\"sentiment\"], ngram=1, max_features=1000, model_random_state=0)\n",
    "top_10 = top_features(forest_model, vectorizer, 10)\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q2a</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q2a passed!"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.b Construct a positive sentiment review\n",
    "\n",
    "**(Total 5 points)**\n",
    "\n",
    "Think of a movie that you like and write a review for it. Store as a string in `good_review`. If the model doesn't give a positive prediction for your review iterate on it until it does.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_b\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not gonna lie I think The Office is a very decent TV show! ⟶ 👍 \u001b[92mcorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "good_review = \"Not gonna lie I think The Office is a very decent TV show!\"\n",
    "check_prediction(forest_model, vectorizer, good_review, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q2_b</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q2_b passed!"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.c Construct a negative sentiment review\n",
    "\n",
    "**(Total 5 points)**\n",
    "\n",
    "Think of a movie that you don't like and write a review for it. Store as a string in `bad_review`. If the model doesn't give a negative prediction for your review iterate on it until it does.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_c\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would never watch Movie XYZ agian... ⟶ 👎 \u001b[92mcorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "bad_review = \"I would never watch Movie XYZ agian...\"\n",
    "check_prediction(forest_model, vectorizer, bad_review, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q2_c</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q2_c passed!"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.d - Construct a misclassified negative sentiment review\n",
    "\n",
    "**(Total 5 points)**\n",
    "\n",
    "Now try to write a review that you view as negative but the model views as positive. Iterate and experiment as necessary and store it as a string  `bad_review_error`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_d\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That movie is fucking disgusting ⟶ 👍 \u001b[31mincorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "bad_review_error = \"That movie is fucking disgusting\"\n",
    "check_prediction(forest_model, vectorizer, bad_review_error, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q2_d</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q2_d passed!"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2_d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2Vec Model\n",
    "**(Total 20 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to train the Word2Vec Model for train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(sentences=[utils.simple_preprocess(review) for review in train['review']], vector_size=100, workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 3.a - Word2Vec similarity\n",
    "\n",
    "**(Total 5 points)**\n",
    "\n",
    "Use the Word2Vec Model we trained above to find 10 similar words for `'actors'`. The result will be a list of words and numbers indicating the similarity of each word. Store the result in `sim`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_a\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actresses', 0.7693343758583069),\n",
       " ('performers', 0.7600549459457397),\n",
       " ('players', 0.7251720428466797),\n",
       " ('cast', 0.6854211688041687),\n",
       " ('performances', 0.6732099652290344),\n",
       " ('roles', 0.6523881554603577),\n",
       " ('names', 0.6495317220687866),\n",
       " ('directors', 0.6385481357574463),\n",
       " ('songs', 0.6271295547485352),\n",
       " ('characters', 0.6236039400100708)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = w2v_model.wv.similar_by_word('actors',10)\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q3_a</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q3_a passed!"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 3.b - Word2Vec doesn't mach\n",
    "\n",
    "**(Total 5 points)**\n",
    "\n",
    "\n",
    "Use the Word2Vec Model we trained above to find the word that doesn't match the others. We will test the words: 'professor', 'engineer', 'scientist', 'cat'. Store the result in `no_match`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_b\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_match = w2v_model.wv.doesnt_match(['professor', 'engineer', 'scientist', 'cat'])\n",
    "no_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q3_b</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q3_b passed!"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c Fit the Word2Vec model\n",
    "\n",
    "**(Total 10 points)**\n",
    "\n",
    "Vector Averaging to get feature encoding of review:\n",
    "\n",
    "\n",
    "One challenge with the IMDB dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.\n",
    "\n",
    "We can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise).\n",
    "\n",
    "The following code averages the feature vectors. You don't need to modify the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_feature_vecs(reviews, model):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one \n",
    "\n",
    "    \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    \n",
    "    reviewFeatureVecs = []\n",
    "    # Loop through the reviews\n",
    "    for counter, review in enumerate(reviews):\n",
    "        \n",
    "        # Print a status message every 5000th review\n",
    "        if (counter + 1) % 5000. == 0.:\n",
    "            print(\"Review %d of %d\" % (counter + 1, len(reviews)))\n",
    "\n",
    "        # Function to average all of the word vectors in a given paragraph\n",
    "        featureVec = []\n",
    "\n",
    "        # Loop over each word in the review and, if it is in the model's\n",
    "        # vocaublary, add its feature vector to the total        \n",
    "        for n,word in enumerate(utils.simple_preprocess(review)):\n",
    "            if word in index2word_set: \n",
    "                featureVec.append(model.wv[word])\n",
    "\n",
    "        \n",
    "        \n",
    "        # Average the word vectors\n",
    "        featureVec = np.mean(featureVec, axis=0).reshape(1,-1)\n",
    "\n",
    "        reviewFeatureVecs.append(featureVec)\n",
    "\n",
    "    return np.concatenate(reviewFeatureVecs, axis=0)\n",
    "\n",
    "w2v_vectorizer = FunctionTransformer(lambda x: get_avg_feature_vecs(x, w2v_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Again, use the function `train_predict_sentiment` to train your random forest model. Remember: we vectorize the text using the Word2Vec model now. Save the trained model in `w2v_forest_model`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_c\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the model!\n",
      "\n",
      "Review 5000 of 20000\n",
      "Review 10000 of 20000\n",
      "Review 15000 of 20000\n",
      "Review 20000 of 20000\n",
      "Review 5000 of 5000\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8038\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2051  497]\n",
      "     pos   [ 484 1968]\n"
     ]
    }
   ],
   "source": [
    "w2v_forest_model = train_predict_sentiment(train['review'], w2v_vectorizer, y=train[\"sentiment\"], ngram=1, max_features=1000, model_random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q3_c</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q3_c passed!"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Word2Vec compares with the Bag of Words Model? Is it an improvement? How significant is the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.d Word2Vec Prediction Analysis\n",
    "\n",
    "**(Total 0 points)**\n",
    "\n",
    "Run the following cells to check to see how the Word2Vec model works on the reviews that you wrote previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not gonna lie I think The Office is a very decent TV show! ⟶ 👍 \u001b[92mcorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "check_prediction(w2v_forest_model, w2v_vectorizer, good_review, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would never watch Movie XYZ agian... ⟶ 👎 \u001b[92mcorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "check_prediction(w2v_forest_model, w2v_vectorizer, bad_review, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Bag of Words:\n",
      "That movie is fucking disgusting ⟶ 👍 \u001b[31mincorrect\u001b[0m\n",
      "With Word2Vec:\n",
      "That movie is fucking disgusting ⟶ 👎 \u001b[92mcorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"With Bag of Words:\")\n",
    "check_prediction(forest_model, vectorizer, bad_review_error, 0)\n",
    "\n",
    "print(\"With Word2Vec:\")\n",
    "check_prediction(w2v_forest_model, w2v_vectorizer, bad_review_error, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the questions below:\n",
    "\n",
    "* Is your positive review classified correctly by Word2Vec?\n",
    "\n",
    "* Is your negative review classified correctly by Word2Vec?\n",
    "\n",
    "* Is your negative review misclassified by Bag of Words now classified correctly by Word2Vec?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The positive review is classified correctly by Word2Vec', 'The negative review is classified correctly by Word2Vec', 'My negative review misclassified by Bag of Words is now classifed correctly by Word2Vec']\n"
     ]
    }
   ],
   "source": [
    "print(['The positive review is classified correctly by Word2Vec', \n",
    "       'The negative review is classified correctly by Word2Vec',\n",
    "       'My negative review misclassified by Bag of Words is now classifed correctly by Word2Vec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to create a pdf for your reference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
